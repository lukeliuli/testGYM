{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "028b26b7-8f9c-4735-97bb-38da740f3ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CartPole-v0\n",
      "(array([ 0.00150361,  0.02905885, -0.00875003, -0.01414045], dtype=float32), {})\n",
      "\n",
      " 0\n",
      "[ 0.00208479 -0.16593653 -0.00903284  0.27576894]\n",
      "<class 'numpy.ndarray'>\n",
      "float32\n",
      "False\n",
      "0 0 1.0 [ 0.00208479 -0.16593653 -0.00903284  0.27576894]\n",
      "(4,)\n",
      "观测空间 = Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "gym.spaces.box.Box\n",
      "float32\n",
      "[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
      "<class 'numpy.ndarray'>\n",
      "(4,)\n",
      "()\n",
      "动作空间 = Discrete(2)\n",
      "gym.spaces.discrete.Discrete\n",
      "int64\n",
      "\n",
      " 1\n",
      "[-0.00123394  0.02931313 -0.00351746 -0.0197492 ]\n",
      "<class 'numpy.ndarray'>\n",
      "float32\n",
      "False\n",
      "0 1 1.0 [-0.00123394  0.02931313 -0.00351746 -0.0197492 ]\n",
      "(4,)\n",
      "观测空间 = Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "gym.spaces.box.Box\n",
      "float32\n",
      "[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
      "<class 'numpy.ndarray'>\n",
      "(4,)\n",
      "()\n",
      "动作空间 = Discrete(2)\n",
      "gym.spaces.discrete.Discrete\n",
      "int64\n",
      "\n",
      " 2\n",
      "[-0.00064768 -0.1657582  -0.00391245  0.27182186]\n",
      "<class 'numpy.ndarray'>\n",
      "float32\n",
      "False\n",
      "0 0 1.0 [-0.00064768 -0.1657582  -0.00391245  0.27182186]\n",
      "(4,)\n",
      "观测空间 = Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "gym.spaces.box.Box\n",
      "float32\n",
      "[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
      "<class 'numpy.ndarray'>\n",
      "(4,)\n",
      "()\n",
      "动作空间 = Discrete(2)\n",
      "gym.spaces.discrete.Discrete\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "测试1，是否GYM能用\n",
    "'''\n",
    "\n",
    "import gym\n",
    " \n",
    "env = gym.make('CartPole-v0',render_mode=None)  # 获取环境\n",
    "# env = gym.make('Pendulum-v0')\n",
    "print(env.spec.id)  # env.spec.id = 'CartPole-v0'\n",
    "#env.render()  # 把环境渲染出来\n",
    " \n",
    "for i_episode in range(1):  # 20个序列\n",
    "    observation = env.reset()  # 初始状态\n",
    "    print(observation)\n",
    "    for t in range(3):  # 每个序列的步长T\n",
    "        #env.render()\n",
    "        print('\\n',t)\n",
    "        action = env.action_space.sample()  # 动作空间随机采样动作\n",
    "        observation, reward, done, info, _ = env.step(action)  # 环境采取动作后的反馈信息\n",
    "        print(observation)\n",
    "        print(type(observation))\n",
    "        print(observation.dtype)\n",
    "        print(info)\n",
    "        print(i_episode, action, reward, observation)  # 打印信息\n",
    "        \n",
    "        print(env.observation_space.shape)\n",
    "        print('观测空间 = {}'.format(env.observation_space))\n",
    "        print(type(env.observation_space))\n",
    "        print(env.observation_space.dtype)\n",
    " \n",
    "        print(env.observation_space.high)\n",
    "        print(type(env.observation_space.high))\n",
    "        print(env.observation_space.high.shape)\n",
    " \n",
    "        print(env.action_space.shape)\n",
    "        print('动作空间 = {}'.format(env.action_space))\n",
    "        print(type(env.action_space))\n",
    "        print(env.action_space.dtype)\n",
    "        if done:  # 若杆的倾斜度数过大，或者车子离初始位置左右的偏离程度过大，或者坚持时间到200帧，则游戏结束\n",
    "            break\n",
    "env.close()  # 结束环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e9401f-e45e-43b3-97f1-3d99c5297115",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "测试2，是否GYM能用,保存为GIF\n",
    "'''\n",
    "\n",
    "import gym\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "frames = []\n",
    "def show():\n",
    "    screen = env.render()\n",
    "    plt.imshow(screen)\n",
    "    plt.show()\n",
    "    \n",
    "env = gym.make('CartPole-v0',render_mode='rgb_array')  # 获取环境\n",
    "# env = gym.make('Pendulum-v0')\n",
    "print(env.spec.id)  # env.spec.id = 'CartPole-v0'\n",
    "\n",
    "print('gym env running') \n",
    "for i_episode in range(1):  # 20个序列\n",
    "    observation = env.reset()  # 初始状态\n",
    "    for t in range(300):  # 每个序列的步长T\n",
    "        frames.append(env.render())\n",
    "        action = env.action_space.sample()  # 动作空间随机采样动作\n",
    "        observation, reward, done, info, _ = env.step(action)  # 环境采取动作后的反馈信息\n",
    "        #show()\n",
    "        if done:  # 若杆的倾斜度数过大，或者车子离初始位置左右的偏离程度过大，或者坚持时间到200帧，则游戏结束\n",
    "            break\n",
    "print('gif writing')\n",
    "with imageio.get_writer('/home/liuli/myCodes/testGYM/gym测试.gif', mode='I') as writer:\n",
    "    for frame in frames:\n",
    "        writer.append_data(frame)\n",
    "env.close()  # 结束环境\n",
    "print('ednded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ca8c45cd-fba4-4971-9802-c110f74710f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在第 10 轮游戏中获得奖励: 10.43 分\n",
      "在第 20 轮游戏中获得奖励: 20.67 分\n",
      "在第 30 轮游戏中获得奖励: 23.70 分\n",
      "在第 40 轮游戏中获得奖励: 20.82 分\n",
      "在第 50 轮游戏中获得奖励: 18.71 分\n",
      "在第 60 轮游戏中获得奖励: 19.10 分\n",
      "在第 70 轮游戏中获得奖励: 30.86 分\n",
      "在第 80 轮游戏中获得奖励: 43.07 分\n",
      "在第 90 轮游戏中获得奖励: 51.59 分\n",
      "在第 100 轮游戏中获得奖励: 94.16 分\n",
      "在第 110 轮游戏中获得奖励: 70.21 分\n",
      "在第 120 轮游戏中获得奖励: 53.52 分\n",
      "在第 130 轮游戏中获得奖励: 48.18 分\n",
      "在第 140 轮游戏中获得奖励: 45.82 分\n",
      "在第 150 轮游戏中获得奖励: 80.54 分\n",
      "在第 160 轮游戏中获得奖励: 123.57 分\n",
      "在第 170 轮游戏中获得奖励: 145.81 分\n",
      "奖励超过 195 ，训练结束\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "测试3，是否tensorflow-keras+gym能用\n",
    "参考\n",
    "https://blog.csdn.net/2401_84495872/article/details/139591541\n",
    "'''\n",
    "\n",
    "import gym # 导入Gym库，用于开发和比较强化学习算法\n",
    "import numpy as np # 导入NumPy库，用于进行科学计算\n",
    "import tensorflow as tf # 导入TensorFlow库\n",
    "from tensorflow import keras # 导入keras模块，这是一个高级神经网络API\n",
    "from tensorflow.keras import layers # 导入keras中的layers模块，用于创建神经网络层\n",
    "import imageio\n",
    "\n",
    "\n",
    "seed = 42 # 设定随机种子，用于复现实验结果\n",
    "gamma = 0.99 # 定义折扣率，用于计算未来奖励的现值\n",
    "max_steps_per_episode = 10000 # 设定每个 episode 的最大步数\n",
    "env = gym.make(\"CartPole-v0\",render_mode='rgb_array') # 创建 CartPole-v0 环境实例\n",
    "\n",
    "eps = np.finfo(np.float32).eps.item() # 获取 float32 数据类型的误差最小值 epsilon \n",
    "\n",
    "#########################\n",
    "num_inputs = 4 # 状态空间的维度，即输入层的节点数\n",
    "num_actions = 2 # 行为空间的维度，即输出层的节点数\n",
    "num_hidden = 128 # 隐藏层的节点数\n",
    "\n",
    "inputs = layers.Input(shape=(num_inputs,)) # 创建输入层，指定输入的形状\n",
    "common = layers.Dense(num_hidden, activation=\"relu\")(inputs) # 创建一个全连接层，包含num_hidden 个神经元，使用 ReLU 作为激活函数\n",
    "action = layers.Dense(num_actions, activation=\"softmax\")(common) # 创建一个全连接层，包含 num_actions 个神经元，使用 softmax 作为激活函数\n",
    "critic = layers.Dense(1)(common) # 创建一个全连接层，包含1个神经元\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=[action, critic]) # 创建一个 Keras 模型，包含输入层、共享的隐藏层和两个输出层\n",
    "\n",
    "\n",
    "#########################\n",
    "\n",
    "    \n",
    "action_probs =np.array([0.5,0.5])    \n",
    "env.reset() \n",
    "frames = []\n",
    "for t in range(max_steps_per_episode):\n",
    "    frames.append(env.render())\n",
    "    observation, reward, done, info, _ = env.step(np.random.choice(num_actions, p=np.squeeze(action_probs)))\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "with imageio.get_writer('/home/liuli/myCodes/testGYM/未训练前的样子.gif', mode='I') as writer:\n",
    "    for frame in frames:\n",
    "        writer.append_data(frame)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "#########################\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.01) # 创建 Adam 优化器实例，设置学习率为 0.01\n",
    "huber_loss = keras.losses.Huber() # 创建损失函数实例\n",
    "action_probs_history = [] # 创建一个列表，用于保存 action 网络在每个步骤中采取各个行动的概率\n",
    "critic_value_history = [] # 创建一个列表，用于保存 critic 网络在每个步骤中对应的值\n",
    "rewards_history = [] # 创建一个列表，用于保存每个步骤的奖励值\n",
    "running_reward = 0 # 初始化运行过程中的每轮奖励\n",
    "episode_count = 0 # 初始化 episode 计数器\n",
    "\n",
    "\n",
    "\n",
    "while True:  \n",
    "    stateInit= env.reset()  # 新一轮游戏开始，重置环境\n",
    "    state = stateInit[0]\n",
    "    episode_reward = 0  # 记录本轮游戏的总奖励值\n",
    "    with tf.GradientTape() as tape:  # 构建 GradientTape 用于计算梯度\n",
    "        for timestep in range(1, max_steps_per_episode): # 本轮游戏如果一切正常会进行 max_steps_per_episode 步\n",
    "            \n",
    "            state = tf.convert_to_tensor(state)  # 将状态转换为张量\n",
    "            \n",
    "            state = tf.expand_dims(state, 0)  # 扩展维度，以适应模型的输入形状\n",
    "            \n",
    "            action_probs, critic_value = model(state)  # 前向传播，得到 action 网络输出的动作空间的概率分布，和 critic 网络预测的奖励值\n",
    "            critic_value_history.append(critic_value[0, 0])  # 将上面 critic 预测的奖励值记录在 critic_value_history 列表中\n",
    "\n",
    "            action = np.random.choice(num_actions, p=np.squeeze(action_probs))  # 依据概率分布抽样某个动作，当然了某个动作概率越大越容易被抽中，同时也保留了一定的随机性\n",
    "            action_probs_history.append(tf.math.log(action_probs[0, action]))  # 将使用该动作的对数概率值记录在 action_probs_history 列表中\n",
    "        \n",
    "            state, reward, done, info, _ = env.step(action)  # 游戏环境使用选中的动作去执行，得到下一个游戏状态、奖励、是否终止和其他信息\n",
    "            rewards_history.append(reward)  # 将该时刻的奖励记录在 rewards_history 列表中\n",
    "            episode_reward += reward  # 累加本轮游戏的总奖励值\n",
    "\n",
    "            if done:  # 如果到达终止状态，则结束循环\n",
    "                break\n",
    "\n",
    "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward  # 计算平均奖励\n",
    "\n",
    "        returns = []  # 存储折扣回报\n",
    "        discounted_sum = 0\n",
    "        for r in rewards_history[::-1]:  # 从后往前遍历奖励的历史值\n",
    "            discounted_sum = r + gamma * discounted_sum  # 计算折扣回报\n",
    "            returns.insert(0, discounted_sum)  # 将折扣回报插入列表的开头，最后形成的还是从前往后的折扣奖励列表\n",
    "\n",
    "        returns = np.array(returns)  # 将折扣回报转换为数组\n",
    "        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)  # 归一化折扣回报\n",
    "        returns = returns.tolist()  # 将折扣回报转换为列表形式\n",
    "\n",
    "        history = zip(action_probs_history, critic_value_history, returns)  # 将三个列表进行 zip 压缩\n",
    "        actor_losses = []  # 存储 action 网络的损失\n",
    "        critic_losses = []  # 存储 critic 网络的损失\n",
    "\n",
    "        for log_prob, value, ret in history:\n",
    "            diff = ret - value\n",
    "            actor_losses.append(-log_prob * diff)  # 计算 actor 的损失函数\n",
    "\n",
    "            critic_losses.append(\n",
    "                huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0)) # 计算 critic 的损失函数\n",
    "            )\n",
    "\n",
    "        loss_value = sum(actor_losses) + sum(critic_losses) # 计算总损失函数\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables) # 计算梯度\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables)) # 更新模型参数\n",
    "\n",
    "        action_probs_history.clear() # 清空之前的历史记录\n",
    "        critic_value_history.clear() # 清空之前的历史记录\n",
    "        rewards_history.clear() # 清空之前的历史记录\n",
    "\n",
    "    episode_count += 1 # 当一轮游戏结束时， episode 加一\n",
    "    if episode_count % 10 == 0: # 每训练 10 个 episode ，输出当前的平均奖励\n",
    "        template = \"在第 {} 轮游戏中获得奖励: {:.2f} 分\"\n",
    "        print(template.format(episode_count, running_reward))\n",
    "\n",
    "    if running_reward > 195:  # 如果平均奖励超过195，视为任务已经解决\n",
    "        print(\"奖励超过 195 ，训练结束\")\n",
    "        break\n",
    "\n",
    "        \n",
    "###################\n",
    "\n",
    "stateInit= env.reset()  # 新一轮游戏开始，重置环境\n",
    "state = stateInit[0]\n",
    "frames = []\n",
    "for t in range(max_steps_per_episode):\n",
    "    frames.append(env.render())\n",
    "    \n",
    "    state = tf.convert_to_tensor(state)  # 将状态转换为张量\n",
    "    state = tf.expand_dims(state, 0)  # 扩展维度，以适应模型的输入形状\n",
    "    \n",
    "    action_probs, _ = model(state)\n",
    "    action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
    "    state, reward, done,info, _ = env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "with imageio.get_writer('/home/liuli/myCodes/testGYM/训练后的样子.gif', mode='I') as writer:\n",
    "    for frame in frames:\n",
    "        writer.append_data(frame)\n",
    "        \n",
    "print('ended')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fa8f45-829e-4e22-b0a7-cb7110608052",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "测试5，GYM+其他算法\n",
    "\n",
    "\n",
    "gym: OpenAI提供的一个用于开发和比较强化学习算法的工具包，可以用于各种任务，包括但不限于环境交互、强化学习等。\n",
    "\n",
    "TensorForce: 一个用于强化学习的库，它提供了基于深度强化学习的算法，并且可以与TensorFlow集成。\n",
    "\n",
    "OpenAI Gym: 一个用于开发和比较强化学习算法的工具包，可以用于各种任务，包括但不限于环境交互、强化学习等。\n",
    "\n",
    "Baselines: 一个提供各种强化学习算法的库，包括DQN、PPO、A2C等。\n",
    "\n",
    "Dopamine: 一个用于研究复杂机器学习系统的开源库，主要关注强化学习。\n",
    "\n",
    "RLlib: 用于分布式强化学习的工具包，可以在大型集群上训练A3C算法。\n",
    "\n",
    "Coach: 由百度提供的，用于强化学习研究的开源框架。\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbb354a6-4839-4df9-a76f-4d2eef436f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"testTensorforce1.py\", line 1, in <module>\n",
      "    from tensorforce import Agent, Environment\n",
      "ImportError: cannot import name 'Agent'\n",
      "Traceback (most recent call last):\n",
      "  File \"testTensorforce2.py\", line 1, in <module>\n",
      "    from tensorforce import Runner\n",
      "ImportError: cannot import name 'Runner'\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "用TensorForce快速搭建深度强化学习模型\n",
    "https://tensorforce.readthedocs.io/en/latest/basics/installation.html\n",
    "1.conda create -n tensorforce python=3.8\n",
    "2.conda activate tensorforce  \n",
    "3.已经下载，并且必须执行，否则会出各种错误\n",
    "git clone https://github.com/tensorforce/tensorforce.git (已经下载)\n",
    "cd tensorforce\n",
    "pip3 install -e . -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "\n",
    "4.conda install ipykernel \n",
    "5.ipython kernel install --user --name=tensorforce\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "!python3 testTensorforce1.py #最简单测试\n",
    "!python3 testTensorforce2.py #测试算法\n",
    "!python3 testTensorforce3.py #看结果+GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74b3ad8-1f32-48fc-b7a3-41c18089d5bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorforce",
   "language": "python",
   "name": "tensorforce"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
