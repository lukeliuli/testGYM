{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028b26b7-8f9c-4735-97bb-38da740f3ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "测试1，是否GYM能用\n",
    "'''\n",
    "\n",
    "import gym\n",
    " \n",
    "env = gym.make('CartPole-v0',render_mode=None)  # 获取环境\n",
    "# env = gym.make('Pendulum-v0')\n",
    "print(env.spec.id)  # env.spec.id = 'CartPole-v0'\n",
    "#env.render()  # 把环境渲染出来\n",
    " \n",
    "for i_episode in range(1):  # 20个序列\n",
    "    observation = env.reset()  # 初始状态\n",
    "    print(observation)\n",
    "    for t in range(3):  # 每个序列的步长T\n",
    "        #env.render()\n",
    "        print('\\n',t)\n",
    "        action = env.action_space.sample()  # 动作空间随机采样动作\n",
    "        observation, reward, done, info, _ = env.step(action)  # 环境采取动作后的反馈信息\n",
    "        print(observation)\n",
    "        print(type(observation))\n",
    "        print(observation.dtype)\n",
    "        print(info)\n",
    "        print(i_episode, action, reward, observation)  # 打印信息\n",
    "        \n",
    "        print(env.observation_space.shape)\n",
    "        print('观测空间 = {}'.format(env.observation_space))\n",
    "        print(type(env.observation_space))\n",
    "        print(env.observation_space.dtype)\n",
    " \n",
    "        print(env.observation_space.high)\n",
    "        print(type(env.observation_space.high))\n",
    "        print(env.observation_space.high.shape)\n",
    " \n",
    "        print(env.action_space.shape)\n",
    "        print('动作空间 = {}'.format(env.action_space))\n",
    "        print(type(env.action_space))\n",
    "        print(env.action_space.dtype)\n",
    "        if done:  # 若杆的倾斜度数过大，或者车子离初始位置左右的偏离程度过大，或者坚持时间到200帧，则游戏结束\n",
    "            break\n",
    "env.close()  # 结束环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e9401f-e45e-43b3-97f1-3d99c5297115",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "测试2，是否GYM能用,保存为GIF\n",
    "'''\n",
    "\n",
    "import gym\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "frames = []\n",
    "def show():\n",
    "    screen = env.render()\n",
    "    plt.imshow(screen)\n",
    "    plt.show()\n",
    "    \n",
    "env = gym.make('CartPole-v0',render_mode='rgb_array')  # 获取环境\n",
    "# env = gym.make('Pendulum-v0')\n",
    "print(env.spec.id)  # env.spec.id = 'CartPole-v0'\n",
    "\n",
    "print('gym env running') \n",
    "for i_episode in range(1):  # 20个序列\n",
    "    observation = env.reset()  # 初始状态\n",
    "    for t in range(300):  # 每个序列的步长T\n",
    "        frames.append(env.render())\n",
    "        action = env.action_space.sample()  # 动作空间随机采样动作\n",
    "        observation, reward, done, info, _ = env.step(action)  # 环境采取动作后的反馈信息\n",
    "        #show()\n",
    "        if done:  # 若杆的倾斜度数过大，或者车子离初始位置左右的偏离程度过大，或者坚持时间到200帧，则游戏结束\n",
    "            break\n",
    "print('gif writing')\n",
    "with imageio.get_writer('/home/liuli/myCodes/testGYM/gym测试.gif', mode='I') as writer:\n",
    "    for frame in frames:\n",
    "        writer.append_data(frame)\n",
    "env.close()  # 结束环境\n",
    "print('ednded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8c45cd-fba4-4971-9802-c110f74710f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "测试3，是否tensorflow-keras+gym能用\n",
    "参考\n",
    "https://blog.csdn.net/2401_84495872/article/details/139591541\n",
    "'''\n",
    "\n",
    "import gym # 导入Gym库，用于开发和比较强化学习算法\n",
    "import numpy as np # 导入NumPy库，用于进行科学计算\n",
    "import tensorflow as tf # 导入TensorFlow库\n",
    "from tensorflow import keras # 导入keras模块，这是一个高级神经网络API\n",
    "from tensorflow.keras import layers # 导入keras中的layers模块，用于创建神经网络层\n",
    "import imageio\n",
    "\n",
    "\n",
    "seed = 42 # 设定随机种子，用于复现实验结果\n",
    "gamma = 0.99 # 定义折扣率，用于计算未来奖励的现值\n",
    "max_steps_per_episode = 10000 # 设定每个 episode 的最大步数\n",
    "env = gym.make(\"CartPole-v0\",render_mode='rgb_array') # 创建 CartPole-v0 环境实例\n",
    "\n",
    "eps = np.finfo(np.float32).eps.item() # 获取 float32 数据类型的误差最小值 epsilon \n",
    "\n",
    "#########################\n",
    "num_inputs = 4 # 状态空间的维度，即输入层的节点数\n",
    "num_actions = 2 # 行为空间的维度，即输出层的节点数\n",
    "num_hidden = 128 # 隐藏层的节点数\n",
    "\n",
    "inputs = layers.Input(shape=(num_inputs,)) # 创建输入层，指定输入的形状\n",
    "common = layers.Dense(num_hidden, activation=\"relu\")(inputs) # 创建一个全连接层，包含num_hidden 个神经元，使用 ReLU 作为激活函数\n",
    "action = layers.Dense(num_actions, activation=\"softmax\")(common) # 创建一个全连接层，包含 num_actions 个神经元，使用 softmax 作为激活函数\n",
    "critic = layers.Dense(1)(common) # 创建一个全连接层，包含1个神经元\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=[action, critic]) # 创建一个 Keras 模型，包含输入层、共享的隐藏层和两个输出层\n",
    "\n",
    "\n",
    "#########################\n",
    "\n",
    "    \n",
    "action_probs =np.array([0.5,0.5])    \n",
    "env.reset() \n",
    "frames = []\n",
    "for t in range(max_steps_per_episode):\n",
    "    frames.append(env.render())\n",
    "    observation, reward, done, info, _ = env.step(np.random.choice(num_actions, p=np.squeeze(action_probs)))\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "with imageio.get_writer('/home/liuli/myCodes/testGYM/未训练前的样子.gif', mode='I') as writer:\n",
    "    for frame in frames:\n",
    "        writer.append_data(frame)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "#########################\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.01) # 创建 Adam 优化器实例，设置学习率为 0.01\n",
    "huber_loss = keras.losses.Huber() # 创建损失函数实例\n",
    "action_probs_history = [] # 创建一个列表，用于保存 action 网络在每个步骤中采取各个行动的概率\n",
    "critic_value_history = [] # 创建一个列表，用于保存 critic 网络在每个步骤中对应的值\n",
    "rewards_history = [] # 创建一个列表，用于保存每个步骤的奖励值\n",
    "running_reward = 0 # 初始化运行过程中的每轮奖励\n",
    "episode_count = 0 # 初始化 episode 计数器\n",
    "\n",
    "\n",
    "\n",
    "while True:  \n",
    "    stateInit= env.reset()  # 新一轮游戏开始，重置环境\n",
    "    state = stateInit[0]\n",
    "    episode_reward = 0  # 记录本轮游戏的总奖励值\n",
    "    with tf.GradientTape() as tape:  # 构建 GradientTape 用于计算梯度\n",
    "        for timestep in range(1, max_steps_per_episode): # 本轮游戏如果一切正常会进行 max_steps_per_episode 步\n",
    "            \n",
    "            state = tf.convert_to_tensor(state)  # 将状态转换为张量\n",
    "            \n",
    "            state = tf.expand_dims(state, 0)  # 扩展维度，以适应模型的输入形状\n",
    "            \n",
    "            action_probs, critic_value = model(state)  # 前向传播，得到 action 网络输出的动作空间的概率分布，和 critic 网络预测的奖励值\n",
    "            critic_value_history.append(critic_value[0, 0])  # 将上面 critic 预测的奖励值记录在 critic_value_history 列表中\n",
    "\n",
    "            action = np.random.choice(num_actions, p=np.squeeze(action_probs))  # 依据概率分布抽样某个动作，当然了某个动作概率越大越容易被抽中，同时也保留了一定的随机性\n",
    "            action_probs_history.append(tf.math.log(action_probs[0, action]))  # 将使用该动作的对数概率值记录在 action_probs_history 列表中\n",
    "        \n",
    "            state, reward, done, info, _ = env.step(action)  # 游戏环境使用选中的动作去执行，得到下一个游戏状态、奖励、是否终止和其他信息\n",
    "            rewards_history.append(reward)  # 将该时刻的奖励记录在 rewards_history 列表中\n",
    "            episode_reward += reward  # 累加本轮游戏的总奖励值\n",
    "\n",
    "            if done:  # 如果到达终止状态，则结束循环\n",
    "                break\n",
    "\n",
    "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward  # 计算平均奖励\n",
    "\n",
    "        returns = []  # 存储折扣回报\n",
    "        discounted_sum = 0\n",
    "        for r in rewards_history[::-1]:  # 从后往前遍历奖励的历史值\n",
    "            discounted_sum = r + gamma * discounted_sum  # 计算折扣回报\n",
    "            returns.insert(0, discounted_sum)  # 将折扣回报插入列表的开头，最后形成的还是从前往后的折扣奖励列表\n",
    "\n",
    "        returns = np.array(returns)  # 将折扣回报转换为数组\n",
    "        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)  # 归一化折扣回报\n",
    "        returns = returns.tolist()  # 将折扣回报转换为列表形式\n",
    "\n",
    "        history = zip(action_probs_history, critic_value_history, returns)  # 将三个列表进行 zip 压缩\n",
    "        actor_losses = []  # 存储 action 网络的损失\n",
    "        critic_losses = []  # 存储 critic 网络的损失\n",
    "\n",
    "        for log_prob, value, ret in history:\n",
    "            diff = ret - value\n",
    "            actor_losses.append(-log_prob * diff)  # 计算 actor 的损失函数\n",
    "\n",
    "            critic_losses.append(\n",
    "                huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0)) # 计算 critic 的损失函数\n",
    "            )\n",
    "\n",
    "        loss_value = sum(actor_losses) + sum(critic_losses) # 计算总损失函数\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables) # 计算梯度\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables)) # 更新模型参数\n",
    "\n",
    "        action_probs_history.clear() # 清空之前的历史记录\n",
    "        critic_value_history.clear() # 清空之前的历史记录\n",
    "        rewards_history.clear() # 清空之前的历史记录\n",
    "\n",
    "    episode_count += 1 # 当一轮游戏结束时， episode 加一\n",
    "    if episode_count % 10 == 0: # 每训练 10 个 episode ，输出当前的平均奖励\n",
    "        template = \"在第 {} 轮游戏中获得奖励: {:.2f} 分\"\n",
    "        print(template.format(episode_count, running_reward))\n",
    "\n",
    "    if running_reward > 195:  # 如果平均奖励超过195，视为任务已经解决\n",
    "        print(\"奖励超过 195 ，训练结束\")\n",
    "        break\n",
    "\n",
    "        \n",
    "###################\n",
    "\n",
    "stateInit= env.reset()  # 新一轮游戏开始，重置环境\n",
    "state = stateInit[0]\n",
    "frames = []\n",
    "for t in range(max_steps_per_episode):\n",
    "    frames.append(env.render())\n",
    "    \n",
    "    state = tf.convert_to_tensor(state)  # 将状态转换为张量\n",
    "    state = tf.expand_dims(state, 0)  # 扩展维度，以适应模型的输入形状\n",
    "    \n",
    "    action_probs, _ = model(state)\n",
    "    action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
    "    state, reward, done,info, _ = env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "with imageio.get_writer('/home/liuli/myCodes/testGYM/训练后的样子.gif', mode='I') as writer:\n",
    "    for frame in frames:\n",
    "        writer.append_data(frame)\n",
    "        \n",
    "print('ended')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fa8f45-829e-4e22-b0a7-cb7110608052",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "测试5，GYM+其他算法\n",
    "\n",
    "\n",
    "gym: OpenAI提供的一个用于开发和比较强化学习算法的工具包，可以用于各种任务，包括但不限于环境交互、强化学习等。\n",
    "\n",
    "TensorForce: 一个用于强化学习的库，它提供了基于深度强化学习的算法，并且可以与TensorFlow集成。\n",
    "\n",
    "OpenAI Gym: 一个用于开发和比较强化学习算法的工具包，可以用于各种任务，包括但不限于环境交互、强化学习等。\n",
    "\n",
    "Baselines: 一个提供各种强化学习算法的库，包括DQN、PPO、A2C等。\n",
    "\n",
    "Dopamine: 一个用于研究复杂机器学习系统的开源库，主要关注强化学习。\n",
    "\n",
    "RLlib: 用于分布式强化学习的工具包，可以在大型集群上训练A3C算法。\n",
    "\n",
    "Coach: 由百度提供的，用于强化学习研究的开源框架。\n",
    "\n",
    "强化学习算法库 stable_baselines3\n",
    "https://zhuanlan.zhihu.com/p/652391991\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb354a6-4839-4df9-a76f-4d2eef436f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "用TensorForce快速搭建深度强化学习模型\n",
    "https://tensorforce.readthedocs.io/en/latest/basics/installation.html\n",
    "1.conda create -n tensorforce python=3.8\n",
    "2.conda activate tensorforce  \n",
    "3.已经下载，并且必须执行，否则会出各种错误\n",
    "git clone https://github.com/tensorforce/tensorforce.git (已经下载)\n",
    "cd tensorforce\n",
    "pip3 install -e . -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "\n",
    "4.conda install ipykernel \n",
    "5.ipython kernel install --user --name=tensorforce\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "!python3 testTensorforce1.py #最简单测试\n",
    "!python3 testTensorforce2.py #测试算法\n",
    "!python3 testTensorforce3.py #看结果+GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74b3ad8-1f32-48fc-b7a3-41c18089d5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##测试自定义Veh2CrashEnv\n",
    "!python3 Veh2CrashEnv.py\n",
    "!python3 mainTestEnvVeh2.py #测试单独测试文件\n",
    "\n",
    "#cd /root/miniconda3/envs/tensorforce/lib/python3.8/site-packages/gym/envs/my_env\n",
    "!python3 mainTesTGymEnvVeh2.py #测试在GYM里面的VehEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba883ce-377d-482c-9bce-790acf2a4568",
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试4，是否tensorflow-keras+自定义gym-Veh2Crash+GPU(环境tensor23py36gpu)能用\n",
    "!python testGymVeh2Gpu.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7d2d6b-e5d7-4bf6-b5ff-89ed9362531a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /root/miniconda3/envs/tensorforce/lib/python3.8/site-packages/gym/envs/my_env/Veh2CrashEnv.py  /home/liuli/myCodes/testGYM/Veh2CrashEnv_tensorforce.py\n",
    "!cp /root/miniconda3/envs/tensor23py36gpu/lib/python3.6/site-packages/gym/envs/my_env/Veh2CrashEnv.py  /home/liuli/myCodes/testGYM/Veh2CrashEnv_tensor23py36gpu.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde40888-0076-48a6-a9cd-141400760fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda list 再tensor23py36gpu\n",
    "udatoolkit               10.1.243            h8cb64d8_11    https://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge\n",
    "cudnn                     7.6.5.32             hc0a50b0_1    https://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge\n",
    "tensorflow-gpu            2.3.1                    pypi_0    pypi\n",
    "keras                     2.3.1\n",
    "gym                       0.26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faa63d0-cb76-4b36-a7c7-1f7f894d4201",
   "metadata": {},
   "outputs": [],
   "source": [
    "!!conda list 在tensorforce\n",
    "tensorflow                2.11.1                   pypi_0    pypi\n",
    "gym                       0.22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d718e547-572e-481b-b9aa-20b2ba08ffae",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "测试5，是否pytorch+自定义gym-Veh2Crash+GPU(环境torch24PY38Gpu)能用\n",
    "'''\n",
    "'''\n",
    "建立环境torch24PY38Gpu\n",
    "!conda create -n torch24PY38Gpu python=3.8\n",
    "#!conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia\n",
    "!pip3 install torch torchvision torchaudio -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "!nvidia-smi\n",
    "!pip install gymnasium==0.29\n",
    "'''\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())  #输出为True，则安装无误\n",
    "!python test5_torchGymVeh2Gpu.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor23py36gpu",
   "language": "python",
   "name": "tensor23py36gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
